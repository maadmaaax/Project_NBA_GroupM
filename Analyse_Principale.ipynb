{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maadmaaax/Project_NBA_GroupM/blob/main/Analyse_Principale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas #requestsfetches the page,beautifulsoup4finds the data on the page, andpandas` organizes it into a table."
      ],
      "metadata": {
        "id": "fytwvNyG-gqu",
        "outputId": "16acbe9a-2943-4900-d597-6ac4c6c96f02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "#Importing all the packages needed"
      ],
      "metadata": {
        "id": "soRvCF4w-nAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/maadmaaax/Project_NBA_GroupM\n",
        "#Load Player stat data set\n",
        "rs2005=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2005.csv')\n",
        "rs2006=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2006.csv')\n",
        "rs2007=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2007.csv')\n",
        "rs2008=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2008.csv')\n",
        "rs2009=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2009.csv')\n",
        "rs2010=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2010.csv')\n",
        "rs2011=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2011.csv')\n",
        "rs2012=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2012.csv')\n",
        "rs2013=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2013.csv')\n",
        "rs2014=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2014.csv')\n",
        "rs2015=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2015.csv')\n",
        "rs2016=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2016.csv')\n",
        "rs2017=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2017.csv')\n",
        "rs2018=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2018.csv')\n",
        "rs2019=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2019.csv')\n",
        "rs2020=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2020.csv')\n",
        "rs2021=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2021.csv')\n",
        "rs2022=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2022.csv')\n",
        "rs2023=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2023.csv')\n",
        "rs2024=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2024.csv')\n",
        "rs2025=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2025.csv')\n",
        "#Load teams result data sets\n",
        "st2005=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2005.csv')\n",
        "st2006=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2006.csv')\n",
        "st2007=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2007.csv')\n",
        "st2008=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2008.csv')\n",
        "st2009=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2009.csv')\n",
        "st2010=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2010.csv')\n",
        "st2011=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2011.csv')\n",
        "st2012=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2012.csv')\n",
        "st2013=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2013.csv')\n",
        "st2014=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2014.csv')\n",
        "st2015=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2015.csv')\n",
        "st2016=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2016.csv')\n",
        "st2017=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2017.csv')\n",
        "st2018=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2018.csv')\n",
        "st2019=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2019.csv')\n",
        "st2020=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2020.csv')\n",
        "st2021=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2021.csv')\n",
        "st2022=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2022.csv')\n",
        "st2023=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2023.csv')\n",
        "st2024=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2024.csv')\n",
        "st2025=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2025.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "j35r5nvdn3L9",
        "outputId": "c0ca5e35-8d06-42fc-d8fe-b8490ebd8834"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Project_NBA_GroupM' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Project_NBA_GroupM/Data/Raw/rs2020.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2435119475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mrs2018\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Project_NBA_GroupM/Data/Raw/rs2018.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mrs2019\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Project_NBA_GroupM/Data/Raw/rs2019.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mrs2020\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Project_NBA_GroupM/Data/Raw/rs2020.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mrs2021\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Project_NBA_GroupM/Data/Raw/rs2021.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mrs2022\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Project_NBA_GroupM/Data/Raw/rs2022.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Project_NBA_GroupM/Data/Raw/rs2020.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing full team name to 3letter code to keep consistent with the rs dataset\n",
        "team_replacements = {\n",
        "    \"Atlanta Hawks\": \"ATL\",\n",
        "    \"Boston Celtics\": \"BOS\",\n",
        "    \"Brooklyn Nets\": \"BRK\",\n",
        "    \"New Jersey Nets\": \"BRK\",\n",
        "    \"NJN\":\"BRK\",\n",
        "    \"Charlotte Hornets\": \"CHO\",\n",
        "    \"Charlotte Bobcats\": \"CHO\",\n",
        "    \"CHA\":\"CHO\",\n",
        "    \"Chicago Bulls\": \"CHI\",\n",
        "    \"Cleveland Cavaliers\": \"CLE\",\n",
        "    \"Dallas Mavericks\": \"DAL\",\n",
        "    \"Denver Nuggets\": \"DEN\",\n",
        "    \"Detroit Pistons\": \"DET\",\n",
        "    \"Golden State Warriors\": \"GSW\",\n",
        "    \"Houston Rockets\": \"HOU\",\n",
        "    \"Indiana Pacers\": \"IND\",\n",
        "    \"Los Angeles Clippers\": \"LAC\",\n",
        "    \"Los Angeles Lakers\": \"LAL\",\n",
        "    \"Memphis Grizzlies\": \"MEM\",\n",
        "    \"Miami Heat\": \"MIA\",\n",
        "    \"Milwaukee Bucks\": \"MIL\",\n",
        "    \"Minnesota Timberwolves\": \"MIN\",\n",
        "    \"New Orleans Pelicans\": \"NOP\",\n",
        "    \"New Orleans Hornets\": \"NOP\",\n",
        "    \"NOH\": \"NOP\",\n",
        "    \"New Orleans/Oklahoma City Hornets\": \"NOP\",\n",
        "    \"NOK\": \"NOP\",\n",
        "    \"New York Knicks\": \"NYK\",\n",
        "    \"Oklahoma City Thunder\": \"OKC\",\n",
        "    \"Seattle SuperSonics\": \"OKC\",\n",
        "    \"SEA\": \"OKC\",\n",
        "    \"Orlando Magic\": \"ORL\",\n",
        "    \"Philadelphia 76ers\": \"PHI\",\n",
        "    \"Phoenix Suns\": \"PHO\",\n",
        "    \"Portland Trail Blazers\": \"POR\",\n",
        "    \"Sacramento Kings\": \"SAC\",\n",
        "    \"San Antonio Spurs\": \"SAS\",\n",
        "    \"Toronto Raptors\": \"TOR\",\n",
        "    \"Utah Jazz\": \"UTA\",\n",
        "    \"Washington Wizards\": \"WAS\"\n",
        "}\n",
        "\n",
        "Dropst=['E','Apr','A','C','SE','NW','P','SW','Post','Pre','Oct','Nov','Dec','Jan','Feb','Mar','Jul','Aug','May']\n",
        "Droprs=['2TM','3TM','TOT','5TM','4TM','nan']\n",
        "years = range(2005, 2026)\n",
        "\n",
        "for year in years:\n",
        "    df_name = f'st{year}'\n",
        "    if df_name in globals():\n",
        "        df = globals()[df_name]\n",
        "        # Strip spaces and replace team names in the relevant column\n",
        "        df['Team'] = df['Team'].str.strip().replace(team_replacements)\n",
        "        df=df.drop(columns=Dropst,errors='ignore')\n",
        "        # Create Win and loss as column and compute %\n",
        "        df[['W','L']]=df['Overall'].str.split('-',expand=True).astype(int)\n",
        "        df['W%']=df['W']/(df['L']+df['W'])\n",
        "        # Save back to variable\n",
        "        globals()[df_name] = df\n",
        "\n",
        "#Changing old team name to recent one for consistency\n",
        "for year in years:\n",
        "    df_name = f'rs{year}'\n",
        "    if df_name in globals():\n",
        "        df = globals()[df_name]\n",
        "        # Strip spaces and replace team names in the relevant column\n",
        "        df['Team'] = df['Team'].str.strip().replace(team_replacements)\n",
        "        df = df[~df.isin(Droprs)]\n",
        "        # Save back to variable\n",
        "        globals()[df_name] = df\n"
      ],
      "metadata": {
        "id": "gQ7GZ57KByEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Exploring Dataframes one by one ---\n",
        "\n",
        "# 1. Create a dictionnary to associate Dataframes names\n",
        "# C'est plus propre pour l'affichage que d'utiliser la liste\n",
        "dfs_dict = {\n",
        "    \"rs2005\": rs2005, \"rs2006\": rs2006, \"rs2007\": rs2007, \"rs2008\": rs2008,\n",
        "    \"rs2009\": rs2009, \"rs2010\": rs2010, \"rs2011\": rs2011, \"rs2012\": rs2012,\n",
        "    \"rs2013\": rs2013, \"rs2014\": rs2014, \"rs2015\": rs2015, \"rs2016\": rs2016,\n",
        "    \"rs2017\": rs2017, \"rs2018\": rs2018, \"rs2019\": rs2019, \"rs2020\": rs2020,\n",
        "    \"rs2021\": rs2021, \"rs2022\": rs2022, \"rs2023\": rs2023, \"rs2024\": rs2024,\n",
        "    \"rs2025\": rs2025\n",
        "}\n",
        "sts_dict = {\n",
        "    \"st2005\": st2005, \"st2006\": st2006, \"st2007\": st2007, \"st2008\": st2008,\n",
        "    \"st2009\": st2009, \"st2010\": st2010, \"st2011\": st2011, \"st2012\": st2012,\n",
        "    \"st2013\": st2013, \"st2014\": st2014, \"st2015\": st2015, \"st2016\": st2016,\n",
        "    \"st2017\": st2017, \"st2018\": st2018, \"st2019\": st2019, \"st2020\": st2020,\n",
        "    \"st2021\": st2021, \"st2022\": st2022, \"st2023\": st2023, \"st2024\": st2024,\n",
        "    \"st2025\": st2025\n",
        "}\n",
        "\n",
        "#print(\"--- Starting the analysis of each seasons ---\")\n",
        "\n",
        "# 2. Loop on each pair (name, DataFrame) in the dictionnairy\n",
        "for name, df in dfs_dict.items():\n",
        "\n",
        "    #print(f\"\\n=========================================\")\n",
        "    #print(f\" ANALYSIS of : {name}\")\n",
        "    #print(f\"=========================================\")\n",
        "\n",
        "    # 1. Display the Shape (Dimensions)\n",
        "    # df.shape[0] = lignes, df.shape[1] = columns (variables)\n",
        "    #print(f\"Dimensions : {df.shape[0]} lines, {df.shape[1]} variables (columns)\")\n",
        "\n",
        "    # 2. Verify the values NaN\n",
        "    # .isnull() creates a DF of booleans (True si NaN)\n",
        "    # .sum() is the sum per column\n",
        "    # .sum() a second sum for the total sum\n",
        "    total_nan = df.isnull().sum().sum()\n",
        "\n",
        "    if total_nan == 0:\n",
        "       # print(\" Good news : No NaN values were found.\")\n",
        "    #else:\n",
        "        # If there is any NaN values, we want to know where\n",
        "       # print(f\" ATTENTION : {total_nan} NaN Values in total were found !\")\n",
        "\n",
        "        # We create a serie of 'nan_per_column' that count the NaN per column\n",
        "        nan_per_column = df.isnull().sum()\n",
        "\n",
        "        # We print only the column that really have NaN\n",
        "       # print(\"Infos of NaN per column :\")\n",
        "       # print(nan_per_column[nan_per_column > 0])\n",
        "\n",
        "    #print(\"-\" * 40)\n",
        "\n",
        "#print(\"\\n--- End of the analysis ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cQXh7IE5s8Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---Display to see howw a dataframes looks like from our database. For example 2005\n",
        "#display(st2005)"
      ],
      "metadata": {
        "id": "4XFWKFY-xiqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rs2006.head(10)"
      ],
      "metadata": {
        "id": "0CV7nhdoyi6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---Combine every seasons from 2005 to 2025 to determine score max & score min\n",
        "#for every variable that are useful in our analysis\n",
        "\n",
        "# 1. Merge all df in a list\n",
        "all_dfs_list = [\n",
        "    rs2005, rs2006, rs2007, rs2008, rs2009, rs2010,\n",
        "    rs2011, rs2012, rs2013, rs2014, rs2015, rs2016,\n",
        "    rs2017, rs2018, rs2019, rs2020, rs2021, rs2022,\n",
        "    rs2023, rs2024, rs2025\n",
        "]\n",
        "all_sts_list = [\n",
        "    st2005, st2006, st2007, st2008, st2009, st2010,\n",
        "    st2011, st2012, st2013, st2014, st2015, st2016,\n",
        "    st2017, st2018, st2019, st2020, st2021, st2022,\n",
        "    st2023, st2024, st2025\n",
        "]\n",
        "\n",
        "#We concat all in one big Dataframe\n",
        "#and we add 'season' column\n",
        "season = 2005\n",
        "for df in all_dfs_list:\n",
        "  df['Season'] = season\n",
        "  season += 1\n",
        "\n",
        "season = 2005\n",
        "for df in all_sts_list:\n",
        "  df['Season'] = season\n",
        "  season += 1\n",
        "\n",
        "sts_data_full = pd.concat(all_sts_list, ignore_index=True)\n",
        "sts_data_full=sts_data_full.drop(columns=Dropst,errors='ignore')\n",
        "nbas_data_full = pd.concat(all_dfs_list, ignore_index=True)\n",
        "nbas_data_full=nbas_data_full[~nbas_data_full.isin(Droprs)]\n",
        "\n",
        "\n",
        "#print(\"All seasons combined\")\n",
        "##print(nbas_data_full.shape)\n",
        "#display(sts_data_full.tail(50))"
      ],
      "metadata": {
        "id": "xm9iqIbE2jov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9e04f55"
      },
      "source": [
        "# Remove the row where 'Rk' is 'League Average'\n",
        "nbas_data_full = nbas_data_full[nbas_data_full['Rk'] != 'League Average'].copy()\n",
        "\n",
        "# List of columns that should be numerical (excluding Player, Team, Pos, Awards, Player-additional)\n",
        "numerical_cols = ['Rk', 'Age', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']\n",
        "\n",
        "# Convert numerical columns to numeric, coercing errors to NaN\n",
        "for col in numerical_cols:\n",
        "    nbas_data_full[col] = pd.to_numeric(nbas_data_full[col], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in the numerical columns\n",
        "nbas_data_full.dropna(subset=numerical_cols, inplace=True)\n",
        "\n",
        "#print(\"DataFrame after removing rows with missing or non-numerical values in specified columns:\")\n",
        "#print(nbas_data_full.shape)\n",
        "#display(nbas_data_full.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "effeaa3e"
      },
      "source": [
        "# Check for missing values in the combined DataFrame\n",
        "missing_values = nbas_data_full.isnull().sum()\n",
        "\n",
        "# Display the number of missing values per column\n",
        "#print(\"Missing values per column in nbas_data_full:\")\n",
        "#display(missing_values[missing_values > 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---2. Creating stats \"Per 36 minutes\"---\n",
        "# We need to normalize the time played per player\n",
        "\n",
        "min_played_col='MP'\n",
        "stats_to_normalize = [\n",
        "    'PTS', 'AST', 'TRB', 'STL', 'BLK', # Positive Stats\n",
        "    'TOV', 'PF' # Negative Stats\n",
        "]\n",
        "\n",
        "#print(f\"Normalisation using'{min_played_col}\")\n",
        "for stat in stats_to_normalize:\n",
        "    new_col_name = f\"{stat}_per_36\"\n",
        "\n",
        "    #For every stats, the formule is : (stat / time played) * 36\n",
        "    # To avoid a zero division we have to use .replace(0,1)\n",
        "    nbas_data_full[new_col_name] = (nbas_data_full[stat] / nbas_data_full[min_played_col].replace(0, 1)) * 36\n",
        "#print(\"Stats 'per 36 minutes' créées.\")\n",
        "#print(nbas_data_full[['Season', min_played_col, 'PTS', 'PTS_per_36']].head())"
      ],
      "metadata": {
        "id": "rcNv7YdaccFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---3. Agregate effeciency stats---\n",
        "# we take new named columns\n",
        "stats_eff = [f\"{stat}_per_36\" for stat in stats_to_normalize]\n",
        "\n",
        "# We group by season & teams & we calculate the mean of stats_eff\n",
        "team_stats_eff = nbas_data_full.groupby(['Season', 'Team'])[stats_eff].mean().reset_index()\n",
        "#print(\"\\nEffeciency mean stats per team per season:\")\n",
        "#display(team_stats_eff.head())\n"
      ],
      "metadata": {
        "id": "ruc22GTtoAR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import positive\n",
        "# --- 4. Power Score Calculation ---\n",
        "team_scores = team_stats_eff[['Season', 'Team']].copy()\n",
        "\n",
        "# Separate positive stats frome the negative ones\n",
        "positive_stats = ['PTS_per_36', 'AST_per_36', 'TRB_per_36', 'STL_per_36', 'BLK_per_36']\n",
        "negative_stats = ['TOV_per_36', 'PF_per_36']\n",
        "\n",
        "#For positive stats\n",
        "for col in positive_stats:\n",
        "    min_score = team_stats_eff[col].min()\n",
        "    max_score = team_stats_eff[col].max()\n",
        "    col_score_name = f\"{col}_score\"\n",
        "    team_scores[col_score_name] = 1 + 5 * (team_stats_eff[col] - min_score) / (max_score - min_score)\n",
        "\n",
        "\n",
        "# For negative stats\n",
        "for col in negative_stats:\n",
        "    min_score = team_stats_eff[col].min()\n",
        "    max_score = team_stats_eff[col].max()\n",
        "    col_score_name = f\"{col}_score\"\n",
        "    team_scores[col_score_name] = 1 + 5 * (max_score - team_stats_eff[col]) / (max_score - min_score)\n",
        "\n",
        "\n",
        "# Calculate the general mean score\n",
        "col_score = [col for col in team_scores.columns if col.endswith('_score')]\n",
        "team_scores['Final_Power_Score'] = team_scores[col_score].mean(axis=1)\n",
        "team_scores['Final_Power_Score']=(team_scores['Final_Power_Score'])/team_scores['Final_Power_Score'].max()\n",
        "\n",
        "# Finding the predictible winners from our model\n",
        "idx_predicts = team_scores.groupby('Season')['Final_Power_Score'].idxmax()\n",
        "\n",
        "pred_winner = team_scores.loc[idx_predicts]\n",
        "#Actual winner of a season\n",
        "idx_winner= sts_data_full.groupby('Season')['W%'].idxmax()\n",
        "idx_winner = idx_winner.dropna()\n",
        "idx_winner=idx_winner.drop(columns=Dropst,errors='ignore')\n",
        "actual_winner=sts_data_full.loc[idx_winner]\n",
        "#Merging relevant columns to make easier comparaison\n",
        "pred1=pred_winner[['Season','Team','Final_Power_Score']]\n",
        "actual1=actual_winner[['Team','W%']]\n",
        "actual1=actual1.rename(columns={'W%':'Win% of actual winner'})\n",
        "#Adding actual rank\n",
        "pred1 = pred1.merge(sts_data_full[['Season','Team','Rk']],\n",
        "                  on=['Season','Team'],\n",
        "                  how='left')\n",
        "\n",
        "\n",
        "pred1 = pred1.rename(columns={'Rk_y': 'Actual Rank'})\n",
        "\n",
        "\n",
        "Comparaison=pd.concat([pred1.reset_index(drop=True),actual1.reset_index(drop=True)],axis=1)\n",
        "Comparaison=Comparaison.rename(columns={'Rk': 'Actual Rank'})\n",
        "\n",
        "print(\"\\n Winners based on our model vs actual winners:\")\n",
        "print(Comparaison)\n"
      ],
      "metadata": {
        "id": "PCSPlSWJqxvr",
        "outputId": "7c85aafb-2962-4d31-c24d-6ab763e31935",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Winners based on our model vs actual winners:\n",
            "    Season Team  Final_Power_Score  Actual Rank Team  Win% of actual winner\n",
            "0     2005  SAC           0.790386            8  PHO               0.756098\n",
            "1     2006  PHO           0.881307            4  DET               0.780488\n",
            "2     2007  DET           0.772769            4  DAL               0.817073\n",
            "3     2008  LAL           0.806611            3  BOS               0.804878\n",
            "4     2009  DET           0.823724           17  CLE               0.804878\n",
            "5     2010  SAS           0.780098           12  CLE               0.743902\n",
            "6     2011  HOU           0.796630           14  CHI               0.756098\n",
            "7     2012  GSW           0.808831           23  CHI               0.757576\n",
            "8     2013  DEN           0.830614            4  MIA               0.804878\n",
            "9     2014  LAL           0.811148           25  SAS               0.756098\n",
            "10    2015  GSW           0.885033            1  GSW               0.817073\n",
            "11    2016  GSW           0.863746            1  GSW               0.890244\n",
            "12    2017  GSW           0.945320            1  GSW               0.817073\n",
            "13    2018  GSW           0.952838            3  HOU               0.792683\n",
            "14    2019  BOS           0.905544            9  MIL               0.731707\n",
            "15    2020  MIL           0.881051            1  MIL               0.767123\n",
            "16    2021  BRK           0.946122            4  UTA               0.722222\n",
            "17    2022  MEM           0.937130            2  PHO               0.780488\n",
            "18    2023  MEM           0.894482            6  MIL               0.707317\n",
            "19    2024  MIN           0.971471            4  BOS               0.780488\n",
            "20    2025  GSW           1.000000           11  OKC               0.829268\n"
          ]
        }
      ]
    }
  ]
}