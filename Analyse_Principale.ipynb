{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maadmaaax/Project_NBA_GroupM/blob/main/Analyse_Principale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas #requestsfetches the page,beautifulsoup4finds the data on the page, andpandas` organizes it into a table."
      ],
      "metadata": {
        "id": "fytwvNyG-gqu",
        "outputId": "2d101dca-0b40-4a57-b318-175b9d4e002f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import statsmodels.api as sm\n",
        "#Importing all the packages needed"
      ],
      "metadata": {
        "id": "soRvCF4w-nAT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/maadmaaax/Project_NBA_GroupM\n",
        "#Load Player stat data set\n",
        "rs2005=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2005.csv')\n",
        "rs2006=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2006.csv')\n",
        "rs2007=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2007.csv')\n",
        "rs2008=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2008.csv')\n",
        "rs2009=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2009.csv')\n",
        "rs2010=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2010.csv')\n",
        "rs2011=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2011.csv')\n",
        "rs2012=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2012.csv')\n",
        "rs2013=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2013.csv')\n",
        "rs2014=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2014.csv')\n",
        "rs2015=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2015.csv')\n",
        "rs2016=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2016.csv')\n",
        "rs2017=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2017.csv')\n",
        "rs2018=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2018.csv')\n",
        "rs2019=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2019.csv')\n",
        "rs2020=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2020.csv')\n",
        "rs2021=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2021.csv')\n",
        "rs2022=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2022.csv')\n",
        "rs2023=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2023.csv')\n",
        "rs2024=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2024.csv')\n",
        "rs2025=pd.read_csv('Project_NBA_GroupM/Data/Raw/rs2025.csv')\n",
        "#Load teams result data sets\n",
        "st2005=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2005.csv')\n",
        "st2006=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2006.csv')\n",
        "st2007=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2007.csv')\n",
        "st2008=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2008.csv')\n",
        "st2009=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2009.csv')\n",
        "st2010=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2010.csv')\n",
        "st2011=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2011.csv')\n",
        "st2012=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2012.csv')\n",
        "st2013=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2013.csv')\n",
        "st2014=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2014.csv')\n",
        "st2015=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2015.csv')\n",
        "st2016=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2016.csv')\n",
        "st2017=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2017.csv')\n",
        "st2018=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2018.csv')\n",
        "st2019=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2019.csv')\n",
        "st2020=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2020.csv')\n",
        "st2021=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2021.csv')\n",
        "st2022=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2022.csv')\n",
        "st2023=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2023.csv')\n",
        "st2024=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2024.csv')\n",
        "st2025=pd.read_csv('Project_NBA_GroupM/Data/Raw/st2025.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j35r5nvdn3L9",
        "outputId": "06343386-ceef-4ac1-d69e-5b47c70c73c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Project_NBA_GroupM'...\n",
            "remote: Enumerating objects: 615, done.\u001b[K\n",
            "remote: Counting objects: 100% (249/249), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 615 (delta 244), reused 225 (delta 225), pack-reused 366 (from 1)\u001b[K\n",
            "Receiving objects: 100% (615/615), 4.21 MiB | 13.98 MiB/s, done.\n",
            "Resolving deltas: 100% (275/275), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing full team name to 3letter code to keep consistent with the rs dataset\n",
        "team_replacements = {\n",
        "    \"Atlanta Hawks\": \"ATL\",\n",
        "    \"Boston Celtics\": \"BOS\",\n",
        "    \"Brooklyn Nets\": \"BRK\",\n",
        "    \"New Jersey Nets\": \"BRK\",\n",
        "    \"NJN\":\"BRK\",\n",
        "    \"Charlotte Hornets\": \"CHO\",\n",
        "    \"Charlotte Bobcats\": \"CHO\",\n",
        "    \"CHA\":\"CHO\",\n",
        "    \"Chicago Bulls\": \"CHI\",\n",
        "    \"Cleveland Cavaliers\": \"CLE\",\n",
        "    \"Dallas Mavericks\": \"DAL\",\n",
        "    \"Denver Nuggets\": \"DEN\",\n",
        "    \"Detroit Pistons\": \"DET\",\n",
        "    \"Golden State Warriors\": \"GSW\",\n",
        "    \"Houston Rockets\": \"HOU\",\n",
        "    \"Indiana Pacers\": \"IND\",\n",
        "    \"Los Angeles Clippers\": \"LAC\",\n",
        "    \"Los Angeles Lakers\": \"LAL\",\n",
        "    \"Memphis Grizzlies\": \"MEM\",\n",
        "    \"Miami Heat\": \"MIA\",\n",
        "    \"Milwaukee Bucks\": \"MIL\",\n",
        "    \"Minnesota Timberwolves\": \"MIN\",\n",
        "    \"New Orleans Pelicans\": \"NOP\",\n",
        "    \"New Orleans Hornets\": \"NOP\",\n",
        "    \"NOH\": \"NOP\",\n",
        "    \"New Orleans/Oklahoma City Hornets\": \"NOP\",\n",
        "    \"NOK\": \"NOP\",\n",
        "    \"New York Knicks\": \"NYK\",\n",
        "    \"Oklahoma City Thunder\": \"OKC\",\n",
        "    \"Seattle SuperSonics\": \"OKC\",\n",
        "    \"SEA\": \"OKC\",\n",
        "    \"Orlando Magic\": \"ORL\",\n",
        "    \"Philadelphia 76ers\": \"PHI\",\n",
        "    \"Phoenix Suns\": \"PHO\",\n",
        "    \"Portland Trail Blazers\": \"POR\",\n",
        "    \"Sacramento Kings\": \"SAC\",\n",
        "    \"San Antonio Spurs\": \"SAS\",\n",
        "    \"Toronto Raptors\": \"TOR\",\n",
        "    \"Utah Jazz\": \"UTA\",\n",
        "    \"Washington Wizards\": \"WAS\"\n",
        "}\n",
        "\n",
        "Dropst=['E','Apr','A','C','SE','NW','P','SW','Post','Pre','Oct','Nov','Dec','Jan','Feb','Mar','Jul','Aug','May']\n",
        "Droprs=['2TM','3TM','TOT','5TM','4TM','nan']\n",
        "years = range(2005, 2026)\n",
        "\n",
        "for year in years:\n",
        "    df_name = f'st{year}'\n",
        "    if df_name in globals():\n",
        "        df = globals()[df_name]\n",
        "        # Strip spaces and replace team names in the relevant column\n",
        "        df['Team'] = df['Team'].str.strip().replace(team_replacements)\n",
        "        df=df.drop(columns=Dropst,errors='ignore')\n",
        "        # Create Win and loss as column and compute %\n",
        "        df[['W','L']]=df['Overall'].str.split('-',expand=True).astype(int)\n",
        "        df['W%']=df['W']/(df['L']+df['W'])\n",
        "        # Save back to variable\n",
        "        globals()[df_name] = df\n",
        "\n",
        "#Changing old team name to recent one for consistency\n",
        "for year in years:\n",
        "    df_name = f'rs{year}'\n",
        "    if df_name in globals():\n",
        "        df = globals()[df_name]\n",
        "        # Strip spaces and replace team names in the relevant column\n",
        "        df['Team'] = df['Team'].str.strip().replace(team_replacements)\n",
        "        df = df[~df.isin(Droprs)]\n",
        "        # Save back to variable\n",
        "        globals()[df_name] = df\n"
      ],
      "metadata": {
        "id": "gQ7GZ57KByEe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Exploring Dataframes one by one ---\n",
        "\n",
        "# 1. Create a dictionnary to associate Dataframes names\n",
        "# C'est plus propre pour l'affichage que d'utiliser la liste\n",
        "dfs_dict = {\n",
        "    \"rs2005\": rs2005, \"rs2006\": rs2006, \"rs2007\": rs2007, \"rs2008\": rs2008,\n",
        "    \"rs2009\": rs2009, \"rs2010\": rs2010, \"rs2011\": rs2011, \"rs2012\": rs2012,\n",
        "    \"rs2013\": rs2013, \"rs2014\": rs2014, \"rs2015\": rs2015, \"rs2016\": rs2016,\n",
        "    \"rs2017\": rs2017, \"rs2018\": rs2018, \"rs2019\": rs2019, \"rs2020\": rs2020,\n",
        "    \"rs2021\": rs2021, \"rs2022\": rs2022, \"rs2023\": rs2023, \"rs2024\": rs2024,\n",
        "    \"rs2025\": rs2025\n",
        "}\n",
        "sts_dict = {\n",
        "    \"st2005\": st2005, \"st2006\": st2006, \"st2007\": st2007, \"st2008\": st2008,\n",
        "    \"st2009\": st2009, \"st2010\": st2010, \"st2011\": st2011, \"st2012\": st2012,\n",
        "    \"st2013\": st2013, \"st2014\": st2014, \"st2015\": st2015, \"st2016\": st2016,\n",
        "    \"st2017\": st2017, \"st2018\": st2018, \"st2019\": st2019, \"st2020\": st2020,\n",
        "    \"st2021\": st2021, \"st2022\": st2022, \"st2023\": st2023, \"st2024\": st2024,\n",
        "    \"st2025\": st2025\n",
        "}\n",
        "\n",
        "#print(\"--- Starting the analysis of each seasons ---\")\n",
        "\n",
        "# 2. Loop on each pair (name, DataFrame) in the dictionnairy\n",
        "for name, df in dfs_dict.items():\n",
        "\n",
        "    #print(f\"\\n=========================================\")\n",
        "    #print(f\" ANALYSIS of : {name}\")\n",
        "    #print(f\"=========================================\")\n",
        "\n",
        "    # 1. Display the Shape (Dimensions)\n",
        "    # df.shape[0] = lignes, df.shape[1] = columns (variables)\n",
        "    #print(f\"Dimensions : {df.shape[0]} lines, {df.shape[1]} variables (columns)\")\n",
        "\n",
        "    # 2. Verify the values NaN\n",
        "    # .isnull() creates a DF of booleans (True si NaN)\n",
        "    # .sum() is the sum per column\n",
        "    # .sum() a second sum for the total sum\n",
        "    total_nan = df.isnull().sum().sum()\n",
        "\n",
        "    if total_nan == 0:\n",
        "       # print(\" Good news : No NaN values were found.\")\n",
        "    #else:\n",
        "        # If there is any NaN values, we want to know where\n",
        "       # print(f\" ATTENTION : {total_nan} NaN Values in total were found !\")\n",
        "\n",
        "        # We create a serie of 'nan_per_column' that count the NaN per column\n",
        "        nan_per_column = df.isnull().sum()\n",
        "\n",
        "        # We print only the column that really have NaN\n",
        "       # print(\"Infos of NaN per column :\")\n",
        "       # print(nan_per_column[nan_per_column > 0])\n",
        "\n",
        "    #print(\"-\" * 40)\n",
        "\n",
        "#print(\"\\n--- End of the analysis ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cQXh7IE5s8Lo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---Display to see howw a dataframes looks like from our database. For example 2005\n",
        "#display(st2005)"
      ],
      "metadata": {
        "id": "4XFWKFY-xiqe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rs2006.head(10)"
      ],
      "metadata": {
        "id": "0CV7nhdoyi6m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---Combine every seasons from 2005 to 2025 to determine score max & score min\n",
        "#for every variable that are useful in our analysis\n",
        "\n",
        "# 1. Merge all df in a list\n",
        "all_dfs_list = [\n",
        "    rs2005, rs2006, rs2007, rs2008, rs2009, rs2010,\n",
        "    rs2011, rs2012, rs2013, rs2014, rs2015, rs2016,\n",
        "    rs2017, rs2018, rs2019, rs2020, rs2021, rs2022,\n",
        "    rs2023, rs2024, rs2025\n",
        "]\n",
        "all_sts_list = [\n",
        "    st2005, st2006, st2007, st2008, st2009, st2010,\n",
        "    st2011, st2012, st2013, st2014, st2015, st2016,\n",
        "    st2017, st2018, st2019, st2020, st2021, st2022,\n",
        "    st2023, st2024, st2025\n",
        "]\n",
        "\n",
        "#We concat all in one big Dataframe\n",
        "#and we add 'season' column\n",
        "season = 2005\n",
        "for df in all_dfs_list:\n",
        "  df['Season'] = season\n",
        "  season += 1\n",
        "\n",
        "season = 2005\n",
        "for df in all_sts_list:\n",
        "  df['Season'] = season\n",
        "  season += 1\n",
        "\n",
        "sts_data_full = pd.concat(all_sts_list, ignore_index=True)\n",
        "sts_data_full=sts_data_full.drop(columns=Dropst,errors='ignore')\n",
        "nbas_data_full = pd.concat(all_dfs_list, ignore_index=True)\n",
        "nbas_data_full=nbas_data_full[~nbas_data_full.isin(Droprs)]\n",
        "\n",
        "\n",
        "#print(\"All seasons combined\")\n",
        "##print(nbas_data_full.shape)\n",
        "#display(sts_data_full.tail(50))"
      ],
      "metadata": {
        "id": "xm9iqIbE2jov"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9e04f55"
      },
      "source": [
        "# Remove the row where 'Rk' is 'League Average'\n",
        "nbas_data_full = nbas_data_full[nbas_data_full['Rk'] != 'League Average'].copy()\n",
        "\n",
        "# List of columns that should be numerical (excluding Player, Team, Pos, Awards, Player-additional)\n",
        "numerical_cols = ['Rk', 'Age', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']\n",
        "\n",
        "# Convert numerical columns to numeric, coercing errors to NaN\n",
        "for col in numerical_cols:\n",
        "    nbas_data_full[col] = pd.to_numeric(nbas_data_full[col], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in the numerical columns\n",
        "nbas_data_full.dropna(subset=numerical_cols, inplace=True)\n",
        "\n",
        "#print(\"DataFrame after removing rows with missing or non-numerical values in specified columns:\")\n",
        "#print(nbas_data_full.shape)\n",
        "#display(nbas_data_full.head())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "effeaa3e"
      },
      "source": [
        "# Check for missing values in the combined DataFrame\n",
        "missing_values = nbas_data_full.isnull().sum()\n",
        "\n",
        "# Display the number of missing values per column\n",
        "#print(\"Missing values per column in nbas_data_full:\")\n",
        "#display(missing_values[missing_values > 0])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Iniating True Shooting percentage TS% to measure a team efficiency\n",
        "nbas_data_full['TS%']=nbas_data_full['PTS']/(2*(nbas_data_full['FGA']+0.44*nbas_data_full['FTA']))\n",
        "# ---2. Creating stats \"Per 36 minutes\"---\n",
        "# We need to normalize the time played per player\n",
        "min_played_col='MP'\n",
        "stats_to_normalize = [\n",
        "    'PTS', 'AST', 'TRB', 'STL', 'BLK', # Positive Stats\n",
        "    'TOV', 'PF' # Negative Stats\n",
        "]\n",
        "\n",
        "#print(f\"Normalisation using'{min_played_col}\")\n",
        "for stat in stats_to_normalize:\n",
        "    new_col_name = f\"{stat}_per_36\"\n",
        "\n",
        "    #For every stats, the formule is : (stat / time played) * 36\n",
        "    # To avoid a zero division we have to use .replace(0,1)\n",
        "    nbas_data_full[new_col_name] = (nbas_data_full[stat] / nbas_data_full[min_played_col].replace(0, 1)) * 36\n",
        "#print(\"Stats 'per 36 minutes' créées.\")\n",
        "#print(nbas_data_full[['Player','Season', min_played_col, 'PTS', 'PTS_per_36','TS%']].head())"
      ],
      "metadata": {
        "id": "rcNv7YdaccFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c2b563-fecf-4f3f-f816-0af48e955357"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Player  Season    MP   PTS  PTS_per_36       TS%\n",
            "0      Allen Iverson    2005  42.3  30.7   26.127660  0.532616\n",
            "1        Kobe Bryant    2005  40.7  27.6   24.412776  0.562256\n",
            "2       LeBron James    2005  42.4  27.2   23.094340  0.552396\n",
            "3      Dirk Nowitzki    2005  38.7  26.1   24.279070  0.579897\n",
            "4  Amar'e Stoudemire    2005  36.1  26.0   25.927978  0.617401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---3. Agregate effeciency stats---\n",
        "# we take new named columns\n",
        "stats_eff = [f\"{stat}_per_36\" for stat in stats_to_normalize]\n",
        "\n",
        "# We group by season & teams & we calculate the mean of stats_eff\n",
        "team_stats_eff = nbas_data_full.groupby(['Season', 'Team'])[stats_eff].mean().reset_index()\n",
        "\n",
        "#print(\"\\nEffeciency mean stats per team per season:\")\n",
        "#display(team_stats_eff.head())\n"
      ],
      "metadata": {
        "id": "ruc22GTtoAR9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import positive\n",
        "# --- 4. Power Score Calculation ---\n",
        "team_scores = team_stats_eff[['Season', 'Team']].copy()\n",
        "\n",
        "# Separate positive stats frome the negative ones\n",
        "positive_stats = ['PTS_per_36', 'AST_per_36', 'TRB_per_36', 'STL_per_36', 'BLK_per_36']\n",
        "negative_stats = ['TOV_per_36', 'PF_per_36']\n",
        "\n",
        "#For positive stats\n",
        "for col in positive_stats:\n",
        "    min_score = team_stats_eff[col].min()\n",
        "    max_score = team_stats_eff[col].max()\n",
        "    col_score_name = f\"{col}_score\"\n",
        "    team_scores[col_score_name] = 1 + 5 * (team_stats_eff[col] - min_score) / (max_score - min_score)\n",
        "\n",
        "\n",
        "# For negative stats\n",
        "for col in negative_stats:\n",
        "    min_score = team_stats_eff[col].min()\n",
        "    max_score = team_stats_eff[col].max()\n",
        "    col_score_name = f\"{col}_score\"\n",
        "    team_scores[col_score_name] = 1 + 5 * (max_score - team_stats_eff[col]) / (max_score - min_score)\n",
        "\n",
        "\n",
        "# Calculate the general mean score\n",
        "col_score = [col for col in team_scores.columns if col.endswith('_score')]\n",
        "team_scores['Final_Power_Score'] = team_scores[col_score].mean(axis=1)\n",
        "team_scores['Final_Power_Score']=(team_scores['Final_Power_Score'])/team_scores['Final_Power_Score'].max()\n",
        "\n",
        "# Finding the predictible winners from our model\n",
        "idx_predicts = team_scores.groupby('Season')['Final_Power_Score'].idxmax()\n",
        "\n",
        "pred_winner = team_scores.loc[idx_predicts]\n",
        "#Actual winner of a season\n",
        "idx_winner= sts_data_full.groupby('Season')['W%'].idxmax()\n",
        "idx_winner = idx_winner.dropna()\n",
        "idx_winner=idx_winner.drop(columns=Dropst,errors='ignore')\n",
        "actual_winner=sts_data_full.loc[idx_winner]\n",
        "#Merging relevant columns to make easier comparaison\n",
        "pred1=pred_winner[['Season','Team','Final_Power_Score']]\n",
        "actual1=actual_winner[['Team','W%']]\n",
        "actual1=actual1.rename(columns={'W%':'Win% of actual winner'})\n",
        "#Adding actual rank\n",
        "pred1 = pred1.merge(sts_data_full[['Season','Team','Rk']],\n",
        "                  on=['Season','Team'],\n",
        "                  how='left')\n",
        "\n",
        "\n",
        "pred1 = pred1.rename(columns={'Rk_y': 'Actual Rank'})\n",
        "\n",
        "\n",
        "Comparaison=pd.concat([pred1.reset_index(drop=True),actual1.reset_index(drop=True)],axis=1)\n",
        "Comparaison=Comparaison.rename(columns={'Rk': 'Actual Rank'})\n",
        "\n",
        "print(\"\\n Winners based on our model vs actual winners:\")\n",
        "print(Comparaison)\n"
      ],
      "metadata": {
        "id": "PCSPlSWJqxvr",
        "outputId": "bbf8582a-ccf2-41f0-af66-453c4de42936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Winners based on our model vs actual winners:\n",
            "    Season Team  Final_Power_Score  Actual Rank Team  Win% of actual winner\n",
            "0     2005  SAC           0.790386            8  PHO               0.756098\n",
            "1     2006  PHO           0.881307            4  DET               0.780488\n",
            "2     2007  DET           0.772769            4  DAL               0.817073\n",
            "3     2008  LAL           0.806611            3  BOS               0.804878\n",
            "4     2009  DET           0.823724           17  CLE               0.804878\n",
            "5     2010  SAS           0.780098           12  CLE               0.743902\n",
            "6     2011  HOU           0.796630           14  CHI               0.756098\n",
            "7     2012  GSW           0.808831           23  CHI               0.757576\n",
            "8     2013  DEN           0.830614            4  MIA               0.804878\n",
            "9     2014  LAL           0.811148           25  SAS               0.756098\n",
            "10    2015  GSW           0.885033            1  GSW               0.817073\n",
            "11    2016  GSW           0.863746            1  GSW               0.890244\n",
            "12    2017  GSW           0.945320            1  GSW               0.817073\n",
            "13    2018  GSW           0.952838            3  HOU               0.792683\n",
            "14    2019  BOS           0.905544            9  MIL               0.731707\n",
            "15    2020  MIL           0.881051            1  MIL               0.767123\n",
            "16    2021  BRK           0.946122            4  UTA               0.722222\n",
            "17    2022  MEM           0.937130            2  PHO               0.780488\n",
            "18    2023  MEM           0.894482            6  MIL               0.707317\n",
            "19    2024  MIN           0.971471            4  BOS               0.780488\n",
            "20    2025  GSW           1.000000           11  OKC               0.829268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.plotting import table\n",
        "#Second approach using regression\n",
        "reg_stat=['PTS', 'AST', 'TRB', 'STL', 'BLK','TOV', 'PF','TS%']\n",
        "reg_stats_Per36=['PTS_per_36', 'AST_per_36', 'TRB_per_36', 'STL_per_36', 'BLK_per_36','TOV_per_36', 'PF_per_36','TS%']\n",
        "all_stats=['PTS', 'AST', 'TRB', 'STL', 'BLK','TOV', 'PF','TS%','PTS_per_36', 'AST_per_36', 'TRB_per_36', 'STL_per_36', 'BLK_per_36','TOV_per_36', 'PF_per_36']\n",
        "# We only include player that have played more than 10minute so player with a significant impact\n",
        "nba_above10=nbas_data_full[nbas_data_full['MP']>10]\n",
        "team_stats_raw=nba_above10.groupby(['Season','Team'])[all_stats].mean().reset_index()\n",
        "regdf= pd.merge(team_stats_raw,sts_data_full,on=['Team','Season'])\n",
        "regdf = regdf.drop(columns=['Rk'])\n",
        "#We created a dataset grouping team average stat and their season records to facilitate the regression\n",
        "\n",
        "model1=sm.OLS(regdf['W%'],regdf[reg_stat]).fit()\n",
        "\n",
        "#print(model1.summary())\n",
        "predict1=model1.predict(regdf[reg_stat])\n",
        "prediction1 = pd.DataFrame(predict1, columns=['Predicted_W%'])\n",
        "#We make a second model using stat per36 instead of regular one\n",
        "model2=sm.OLS(regdf['W%'],regdf[reg_stats_Per36]).fit()\n",
        "predict2=model2.predict(regdf[reg_stat])\n",
        "prediction2 = pd.DataFrame(predict2, columns=['Predicted_W%'])\n",
        "# Reset the index of regdf before merging to ensure proper alignment if indices are not unique\n",
        "regdf_reset = regdf.reset_index(drop=True)\n",
        "\n",
        "# Concatenate prediction1 with the relevant columns from regdf\n",
        "result1 = pd.concat([regdf_reset[['Season', 'Team', 'W%']], prediction1], axis=1)\n",
        "result2 = pd.concat([regdf_reset[['Season', 'Team', 'W%']], prediction2], axis=1)\n",
        "\n",
        "\n",
        "print(model1.summary())\n",
        "print(model2.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j94Xn9wgETtN",
        "outputId": "4a0c5740-5a7f-4122-f4dc-c3127e52c090"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 OLS Regression Results                                \n",
            "=======================================================================================\n",
            "Dep. Variable:                     W%   R-squared (uncentered):                   0.940\n",
            "Model:                            OLS   Adj. R-squared (uncentered):              0.940\n",
            "Method:                 Least Squares   F-statistic:                              1226.\n",
            "Date:                Mon, 24 Nov 2025   Prob (F-statistic):                        0.00\n",
            "Time:                        14:45:35   Log-Likelihood:                          403.72\n",
            "No. Observations:                 630   AIC:                                     -791.4\n",
            "Df Residuals:                     622   BIC:                                     -755.9\n",
            "Df Model:                           8                                                  \n",
            "Covariance Type:            nonrobust                                                  \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "PTS            0.0396      0.009      4.190      0.000       0.021       0.058\n",
            "AST            0.0114      0.025      0.455      0.649      -0.038       0.061\n",
            "TRB           -0.0108      0.020     -0.545      0.586      -0.050       0.028\n",
            "STL            0.2350      0.061      3.854      0.000       0.115       0.355\n",
            "BLK            0.3236      0.069      4.719      0.000       0.189       0.458\n",
            "TOV           -0.2924      0.044     -6.673      0.000      -0.378      -0.206\n",
            "PF            -0.0567      0.032     -1.800      0.072      -0.119       0.005\n",
            "TS%            0.5555      0.130      4.264      0.000       0.300       0.811\n",
            "==============================================================================\n",
            "Omnibus:                       10.685   Durbin-Watson:                   1.874\n",
            "Prob(Omnibus):                  0.005   Jarque-Bera (JB):                6.652\n",
            "Skew:                          -0.069   Prob(JB):                       0.0359\n",
            "Kurtosis:                       2.516   Cond. No.                         296.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
            "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "                                 OLS Regression Results                                \n",
            "=======================================================================================\n",
            "Dep. Variable:                     W%   R-squared (uncentered):                   0.941\n",
            "Model:                            OLS   Adj. R-squared (uncentered):              0.940\n",
            "Method:                 Least Squares   F-statistic:                              1230.\n",
            "Date:                Mon, 24 Nov 2025   Prob (F-statistic):                        0.00\n",
            "Time:                        14:45:35   Log-Likelihood:                          404.66\n",
            "No. Observations:                 630   AIC:                                     -793.3\n",
            "Df Residuals:                     622   BIC:                                     -757.7\n",
            "Df Model:                           8                                                  \n",
            "Covariance Type:            nonrobust                                                  \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "PTS_per_36    -0.0241      0.008     -3.016      0.003      -0.040      -0.008\n",
            "AST_per_36    -0.0155      0.016     -0.987      0.324      -0.046       0.015\n",
            "TRB_per_36    -0.0335      0.011     -2.913      0.004      -0.056      -0.011\n",
            "STL_per_36     0.0686      0.039      1.759      0.079      -0.008       0.145\n",
            "BLK_per_36     0.1158      0.044      2.610      0.009       0.029       0.203\n",
            "TOV_per_36    -0.1621      0.027     -6.091      0.000      -0.214      -0.110\n",
            "PF_per_36     -0.0583      0.019     -3.066      0.002      -0.096      -0.021\n",
            "TS%            2.7056      0.244     11.075      0.000       2.226       3.185\n",
            "==============================================================================\n",
            "Omnibus:                        1.885   Durbin-Watson:                   1.918\n",
            "Prob(Omnibus):                  0.390   Jarque-Bera (JB):                1.783\n",
            "Skew:                          -0.055   Prob(JB):                        0.410\n",
            "Kurtosis:                       2.763   Cond. No.                         803.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
            "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    }
  ]
}